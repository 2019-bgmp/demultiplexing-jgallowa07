#!/bin/bash
#SBATCH --account=bgmp          ### SLURM account which will be charged for the job
#SBATCH --partition=bgmp        ### Partition (like a queue in PBS)
#SBATCH --job-name=demultiplexing_distribution      ### Job Name
#SBATCH --output=slurm_output/dm_dist.out         ### File in which to store job output
#SBATCH --error=slurm_output/dm_dist.err          ### File in which to store job error messages
#SBATCH --time=1-00:00:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --nodes=1               ### Node count required for the job (usually 1)
#SBATCH --ntasks-per-node=1     ### Nuber of tasks to be launched per Node (usually 1)
#SBATCH --cpus-per-task=8       ### Number of cpus (cores) per task
#SBATCH --mail-user=jaredgalloway07@gmail.com
#SBATCH --mail-type=ALL

# Load modules


# SCRIPT BELOW
# /usr/bin/time -v./P1.py -f emp_files/1294_S1_L008_R1_001.fastq.gz -nbp 101
# /usr/bin/time -v./P1.py -f emp_files/1294_S1_L008_R2_001.fastq.gz -nbp 8
# /usr/bin/time -v ./P1.py -f emp_files/1294_S1_L008_R3_001.fastq.gz -nbp 8
# /usr/bin/time -v ./P1.py -f emp_files/1294_S1_L008_R4_001.fastq.gz -nbp 101

# /usr/bin/time -v zcat emp_files/1294_S1_L008_R[2-3]* | awk 'NR%4==2' | grep N | wc -l > countN_Index.txt

/usr/bin/time -v ./P2.py -fq emp_files/1294_S1_L008_R* -fi emp_files/indexes.txt


